Namespace(data_dir='../tsp_input/generated_insatnces_3000_size_50/', tb_dir='../atsp_model_train_result', atsp_size=50, to_homo=False, half_st=False, model='HetroGATConcat', input_dim=1, hidden_dim=32, output_dim=1, relation_types='ss tt pp', n_gnn_layers=1, n_heads=1, jk='cat', lr_init=0.001, lr_decay=0.95, min_delta=0.0001, patience=200, batch_size=15, n_epochs=1, checkpoint_freq=10, seed=4, n_trials=1, n_samples_result_train=30, device='cuda')
Namespace(atsp_size=1000, data_path='../tsp_input/generated_insatnces_100_size_1000', model_path='../atsp_model_train_result/Oct20_14-51-21_HetroGATConcat_trained_ATSP50/trial_0', time_limit=0.16, perturbation_moves=5, device='cuda')
before uploading the test
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.3.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
  warnings.warn(
after
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
model: HetroGATConcat trained in ATSP1000 for 1 and tested in ATSP1000 for 30
before the model
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
Using 2 GPUs!
after the model
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
device = cuda
Traceback (most recent call last):
  File "/project/p_gnn001/code/tsp/tsp_gnn/test.py", line 178, in <module>
    main(args_test)
  File "/project/p_gnn001/code/tsp/tsp_gnn/test.py", line 53, in main
    model.load_state_dict(checkpoint['model_state_dict'])
  File "/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2584, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for DataParallel:
	Missing key(s) in state_dict: "module.embed_layer.linears.0.weight", "module.embed_layer.linears.0.bias", "module.embed_layer.linears.1.weight", "module.embed_layer.linears.1.bias", "module.embed_layer.batch_norm.weight", "module.embed_layer.batch_norm.bias", "module.embed_layer.batch_norm.running_mean", "module.embed_layer.batch_norm.running_var", "module.gnn_layers.0.mods.ss.attn_l", "module.gnn_layers.0.mods.ss.attn_r", "module.gnn_layers.0.mods.ss.bias", "module.gnn_layers.0.mods.ss.fc.weight", "module.gnn_layers.0.mods.tt.attn_l", "module.gnn_layers.0.mods.tt.attn_r", "module.gnn_layers.0.mods.tt.bias", "module.gnn_layers.0.mods.tt.fc.weight", "module.gnn_layers.0.mods.pp.attn_l", "module.gnn_layers.0.mods.pp.attn_r", "module.gnn_layers.0.mods.pp.bias", "module.gnn_layers.0.mods.pp.fc.weight", "module.gnn_layers.1.mods.ss.attn_l", "module.gnn_layers.1.mods.ss.attn_r", "module.gnn_layers.1.mods.ss.bias", "module.gnn_layers.1.mods.ss.fc.weight", "module.gnn_layers.1.mods.tt.attn_l", "module.gnn_layers.1.mods.tt.attn_r", "module.gnn_layers.1.mods.tt.bias", "module.gnn_layers.1.mods.tt.fc.weight", "module.gnn_layers.1.mods.pp.attn_l", "module.gnn_layers.1.mods.pp.attn_r", "module.gnn_layers.1.mods.pp.bias", "module.gnn_layers.1.mods.pp.fc.weight", "module.gnn_layers.2.mods.ss.attn_l", "module.gnn_layers.2.mods.ss.attn_r", "module.gnn_layers.2.mods.ss.bias", "module.gnn_layers.2.mods.ss.fc.weight", "module.gnn_layers.2.mods.tt.attn_l", "module.gnn_layers.2.mods.tt.attn_r", "module.gnn_layers.2.mods.tt.bias", "module.gnn_layers.2.mods.tt.fc.weight", "module.gnn_layers.2.mods.pp.attn_l", "module.gnn_layers.2.mods.pp.attn_r", "module.gnn_layers.2.mods.pp.bias", "module.gnn_layers.2.mods.pp.fc.weight", "module.gnn_layers.3.mods.ss.attn_l", "module.gnn_layers.3.mods.ss.attn_r", "module.gnn_layers.3.mods.ss.bias", "module.gnn_layers.3.mods.ss.fc.weight", "module.gnn_layers.3.mods.tt.attn_l", "module.gnn_layers.3.mods.tt.attn_r", "module.gnn_layers.3.mods.tt.bias", "module.gnn_layers.3.mods.tt.fc.weight", "module.gnn_layers.3.mods.pp.attn_l", "module.gnn_layers.3.mods.pp.attn_r", "module.gnn_layers.3.mods.pp.bias", "module.gnn_layers.3.mods.pp.fc.weight", "module.mlp_layers.0.linears.0.weight", "module.mlp_layers.0.linears.0.bias", "module.mlp_layers.0.linears.1.weight", "module.mlp_layers.0.linears.1.bias", "module.mlp_layers.0.batch_norm.weight", "module.mlp_layers.0.batch_norm.bias", "module.mlp_layers.0.batch_norm.running_mean", "module.mlp_layers.0.batch_norm.running_var", "module.mlp_layers.1.linears.0.weight", "module.mlp_layers.1.linears.0.bias", "module.mlp_layers.1.linears.1.weight", "module.mlp_layers.1.linears.1.bias", "module.mlp_layers.1.batch_norm.weight", "module.mlp_layers.1.batch_norm.bias", "module.mlp_layers.1.batch_norm.running_mean", "module.mlp_layers.1.batch_norm.running_var", "module.mlp_layers.2.linears.0.weight", "module.mlp_layers.2.linears.0.bias", "module.mlp_layers.2.linears.1.weight", "module.mlp_layers.2.linears.1.bias", "module.mlp_layers.2.batch_norm.weight", "module.mlp_layers.2.batch_norm.bias", "module.mlp_layers.2.batch_norm.running_mean", "module.mlp_layers.2.batch_norm.running_var", "module.mlp_layers.3.linears.0.weight", "module.mlp_layers.3.linears.0.bias", "module.mlp_layers.3.linears.1.weight", "module.mlp_layers.3.linears.1.bias", "module.mlp_layers.3.batch_norm.weight", "module.mlp_layers.3.batch_norm.bias", "module.mlp_layers.3.batch_norm.running_mean", "module.mlp_layers.3.batch_norm.running_var", "module.decision_layer.linears.0.weight", "module.decision_layer.linears.0.bias", "module.decision_layer.linears.1.weight", "module.decision_layer.linears.1.bias", "module.decision_layer.batch_norm.weight", "module.decision_layer.batch_norm.bias", "module.decision_layer.batch_norm.running_mean", "module.decision_layer.batch_norm.running_var". 
	Unexpected key(s) in state_dict: "embed_layer.linears.0.weight", "embed_layer.linears.0.bias", "embed_layer.linears.1.weight", "embed_layer.linears.1.bias", "embed_layer.batch_norm.weight", "embed_layer.batch_norm.bias", "embed_layer.batch_norm.running_mean", "embed_layer.batch_norm.running_var", "embed_layer.batch_norm.num_batches_tracked", "gnn_layers.0.mods.ss.attn_l", "gnn_layers.0.mods.ss.attn_r", "gnn_layers.0.mods.ss.bias", "gnn_layers.0.mods.ss.fc.weight", "gnn_layers.0.mods.tt.attn_l", "gnn_layers.0.mods.tt.attn_r", "gnn_layers.0.mods.tt.bias", "gnn_layers.0.mods.tt.fc.weight", "gnn_layers.0.mods.pp.attn_l", "gnn_layers.0.mods.pp.attn_r", "gnn_layers.0.mods.pp.bias", "gnn_layers.0.mods.pp.fc.weight", "gnn_layers.1.mods.ss.attn_l", "gnn_layers.1.mods.ss.attn_r", "gnn_layers.1.mods.ss.bias", "gnn_layers.1.mods.ss.fc.weight", "gnn_layers.1.mods.tt.attn_l", "gnn_layers.1.mods.tt.attn_r", "gnn_layers.1.mods.tt.bias", "gnn_layers.1.mods.tt.fc.weight", "gnn_layers.1.mods.pp.attn_l", "gnn_layers.1.mods.pp.attn_r", "gnn_layers.1.mods.pp.bias", "gnn_layers.1.mods.pp.fc.weight", "gnn_layers.2.mods.ss.attn_l", "gnn_layers.2.mods.ss.attn_r", "gnn_layers.2.mods.ss.bias", "gnn_layers.2.mods.ss.fc.weight", "gnn_layers.2.mods.tt.attn_l", "gnn_layers.2.mods.tt.attn_r", "gnn_layers.2.mods.tt.bias", "gnn_layers.2.mods.tt.fc.weight", "gnn_layers.2.mods.pp.attn_l", "gnn_layers.2.mods.pp.attn_r", "gnn_layers.2.mods.pp.bias", "gnn_layers.2.mods.pp.fc.weight", "gnn_layers.3.mods.ss.attn_l", "gnn_layers.3.mods.ss.attn_r", "gnn_layers.3.mods.ss.bias", "gnn_layers.3.mods.ss.fc.weight", "gnn_layers.3.mods.tt.attn_l", "gnn_layers.3.mods.tt.attn_r", "gnn_layers.3.mods.tt.bias", "gnn_layers.3.mods.tt.fc.weight", "gnn_layers.3.mods.pp.attn_l", "gnn_layers.3.mods.pp.attn_r", "gnn_layers.3.mods.pp.bias", "gnn_layers.3.mods.pp.fc.weight", "mlp_layers.0.linears.0.weight", "mlp_layers.0.linears.0.bias", "mlp_layers.0.linears.1.weight", "mlp_layers.0.linears.1.bias", "mlp_layers.0.batch_norm.weight", "mlp_layers.0.batch_norm.bias", "mlp_layers.0.batch_norm.running_mean", "mlp_layers.0.batch_norm.running_var", "mlp_layers.0.batch_norm.num_batches_tracked", "mlp_layers.1.linears.0.weight", "mlp_layers.1.linears.0.bias", "mlp_layers.1.linears.1.weight", "mlp_layers.1.linears.1.bias", "mlp_layers.1.batch_norm.weight", "mlp_layers.1.batch_norm.bias", "mlp_layers.1.batch_norm.running_mean", "mlp_layers.1.batch_norm.running_var", "mlp_layers.1.batch_norm.num_batches_tracked", "mlp_layers.2.linears.0.weight", "mlp_layers.2.linears.0.bias", "mlp_layers.2.linears.1.weight", "mlp_layers.2.linears.1.bias", "mlp_layers.2.batch_norm.weight", "mlp_layers.2.batch_norm.bias", "mlp_layers.2.batch_norm.running_mean", "mlp_layers.2.batch_norm.running_var", "mlp_layers.2.batch_norm.num_batches_tracked", "mlp_layers.3.linears.0.weight", "mlp_layers.3.linears.0.bias", "mlp_layers.3.linears.1.weight", "mlp_layers.3.linears.1.bias", "mlp_layers.3.batch_norm.weight", "mlp_layers.3.batch_norm.bias", "mlp_layers.3.batch_norm.running_mean", "mlp_layers.3.batch_norm.running_var", "mlp_layers.3.batch_norm.num_batches_tracked", "decision_layer.linears.0.weight", "decision_layer.linears.0.bias", "decision_layer.linears.1.weight", "decision_layer.linears.1.bias", "decision_layer.batch_norm.weight", "decision_layer.batch_norm.bias", "decision_layer.batch_norm.running_mean", "decision_layer.batch_norm.running_var", "decision_layer.batch_norm.num_batches_tracked". 

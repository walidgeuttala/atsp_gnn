Namespace(data_dir='../tsp_input/generated_insatnces_3000_size_50/', tb_dir='../atsp_model_train_result', atsp_size=50, to_homo=False, half_st=False, model='HetroGATConcat', input_dim=1, hidden_dim=128, output_dim=1, relation_types='ss tt pp', n_gnn_layers=4, n_heads=64, jk='cat', lr_init=0.001, lr_decay=0.95, min_delta=0.0001, patience=200, batch_size=15, n_epochs=100, checkpoint_freq=10, seed=4, n_trials=1, n_samples_result_train=30, device='cuda')
Namespace(atsp_size=1000, data_path='../tsp_input/generated_insatnces_100_size_1000', model_path='../atsp_model_train_result/Oct17_04-09-53_HetroGATConcat_trained_ATSP50/trial_0', time_limit=0.16, perturbation_moves=5, device='cuda')
before uploading the test
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
after
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
model: HetroGATConcat trained in ATSP1000 for 100 and tested in ATSP1000 for 30
before the model
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
after the model
GPU Memory Allocated: 2.42 MB
GPU Memory Cached: 4.00 MB
GPU Memory Free: 40370.83 MB
Total GPU Memory: 40377.25 MB
device = cuda
after the loading weights
GPU Memory Allocated: 9.71 MB
GPU Memory Cached: 10.00 MB
GPU Memory Free: 40357.54 MB
Total GPU Memory: 40377.25 MB
  0%|          | 0/30 [00:00<?, ?it/s]before loading the first sample
GPU Memory Allocated: 9.71 MB
GPU Memory Cached: 10.00 MB
GPU Memory Free: 40357.54 MB
Total GPU Memory: 40377.25 MB
after
GPU Memory Allocated: 15237.15 MB
GPU Memory Cached: 15246.00 MB
GPU Memory Free: 9894.10 MB
Total GPU Memory: 40377.25 MB
  0%|          | 0/30 [00:34<?, ?it/s]
Traceback (most recent call last):
  File "/project/p_gnn001/code/tsp/tsp_gnn/test.py", line 180, in <module>
    main(args_test)
  File "/project/p_gnn001/code/tsp/tsp_gnn/test.py", line 90, in main
    y_pred = model(H, x)
             ^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/p_gnn001/code/tsp/tsp_gnn/gnngls/model.py", line 69, in forward
    h2 = gnn_layer(graph, h1)
         ^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/nn/pytorch/hetero.py", line 210, in forward
    dstdata = self._get_module((stype, etype, dtype))(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/nn/pytorch/conv/gatconv.py", line 337, in forward
    graph.apply_edges(fn.u_add_v("el", "er", "e"))
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/heterograph.py", line 4698, in apply_edges
    edata = core.invoke_gsddmm(g, func)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/core.py", line 297, in invoke_gsddmm
    z = op(graph, x, y)
        ^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/ops/sddmm.py", line 137, in func
    return gsddmm(
           ^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/ops/sddmm.py", line 78, in gsddmm
    return gsddmm_internal(
           ^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/backend/pytorch/sparse.py", line 1046, in gsddmm
    return GSDDMM.apply(*args)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/backend/pytorch/sparse.py", line 446, in forward
    out = _gsddmm(gidx, op, X, Y, lhs_target, rhs_target)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/_sparse_ops.py", line 552, in _gsddmm
    out = F.empty(out_shp, dtype, ctx)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/dgl/backend/pytorch/tensor.py", line 284, in empty
    return th.empty(shape, dtype=dtype, device=ctx)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 59.43 GiB. GPU 0 has a total capacity of 39.43 GiB of which 20.52 GiB is free. Including non-PyTorch memory, this process has 18.91 GiB memory in use. Of the allocated memory 16.92 GiB is allocated by PyTorch, and 478.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

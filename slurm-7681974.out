Namespace(data_dir='../tsp_input/generated_insatnces_3000_size_50/', tb_dir='../atsp_model_train_result', atsp_size=50, to_homo=False, half_st=False, model='HetroGATConcat', input_dim=1, hidden_dim=32, output_dim=1, relation_types='ss tt pp', n_gnn_layers=1, n_heads=1, jk='cat', lr_init=0.001, lr_decay=0.95, min_delta=0.0001, patience=200, batch_size=15, n_epochs=1, checkpoint_freq=10, seed=4, n_trials=1, n_samples_result_train=30, device='cuda')
Namespace(atsp_size=1000, data_path='../tsp_input/generated_insatnces_100_size_1000', model_path='../atsp_model_train_result/Oct20_14-51-21_HetroGATConcat_trained_ATSP50/trial_0', time_limit=0.16, perturbation_moves=5, device='cuda')
before uploading the test
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.3.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:
https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations
  warnings.warn(
after
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
model: HetroGATConcat trained in ATSP1000 for 1 and tested in ATSP1000 for 30
before the model
GPU Memory Allocated: 0.00 MB
GPU Memory Cached: 0.00 MB
GPU Memory Free: 40377.25 MB
Total GPU Memory: 40377.25 MB
after the model
GPU Memory Allocated: 0.19 MB
GPU Memory Cached: 2.00 MB
GPU Memory Free: 40375.06 MB
Total GPU Memory: 40377.25 MB
device = cuda
after the loading weights
GPU Memory Allocated: 0.77 MB
GPU Memory Cached: 2.00 MB
GPU Memory Free: 40374.48 MB
Total GPU Memory: 40377.25 MB
  0%|          | 0/30 [00:00<?, ?it/s]before loading the first sample
GPU Memory Allocated: 0.77 MB
GPU Memory Cached: 2.00 MB
GPU Memory Free: 40374.48 MB
Total GPU Memory: 40377.25 MB
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   4695 KiB |   4695 KiB |   4695 KiB |      0 B   |
|       from large pool |   3902 KiB |   3902 KiB |   3902 KiB |      0 B   |
|       from small pool |    793 KiB |    793 KiB |    793 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |   4695 KiB |   4695 KiB |   4695 KiB |      0 B   |
|       from large pool |   3902 KiB |   3902 KiB |   3902 KiB |      0 B   |
|       from small pool |    793 KiB |    793 KiB |    793 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |   4548 KiB |   4548 KiB |   4548 KiB |      0 B   |
|       from large pool |   3902 KiB |   3902 KiB |   3902 KiB |      0 B   |
|       from small pool |    645 KiB |    645 KiB |    645 KiB |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  22528 KiB |  22528 KiB |  22528 KiB |      0 B   |
|       from large pool |  20480 KiB |  20480 KiB |  20480 KiB |      0 B   |
|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |  17832 KiB |  17832 KiB |  18625 KiB |    792 KiB |
|       from large pool |  16577 KiB |  16577 KiB |  16577 KiB |      0 KiB |
|       from small pool |   1255 KiB |   2047 KiB |   2047 KiB |    792 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     459    |     459    |     459    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |     458    |     458    |     458    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     459    |     459    |     459    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |     458    |     458    |     458    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |       2    |       2    |       0    |
|       from large pool |       1    |       1    |       1    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

after
GPU Memory Allocated: 4.59 MB
GPU Memory Cached: 22.00 MB
GPU Memory Free: 40350.66 MB
Total GPU Memory: 40377.25 MB
in the model before the local scope
GPU Memory Allocated: 4.59 MB
GPU Memory Cached: 22.00 MB
GPU Memory Free: 40350.66 MB
Total GPU Memory: 40377.25 MB
1
GPU Memory Allocated: 4.59 MB
GPU Memory Cached: 22.00 MB
GPU Memory Free: 40350.66 MB
Total GPU Memory: 40377.25 MB
2
GPU Memory Allocated: 134.71 MB
GPU Memory Cached: 388.00 MB
GPU Memory Free: 39854.54 MB
Total GPU Memory: 40377.25 MB
3
GPU Memory Allocated: 134.71 MB
GPU Memory Cached: 388.00 MB
GPU Memory Free: 39854.54 MB
Total GPU Memory: 40377.25 MB
  0%|          | 0/30 [00:25<?, ?it/s]
Traceback (most recent call last):
  File "/project/p_gnn001/code/tsp/tsp_gnn/test.py", line 175, in <module>
    main(args_test)
  File "/project/p_gnn001/code/tsp/tsp_gnn/test.py", line 82, in main
    y_pred = model(H, x)
             ^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/p_gnn001/code/tsp/tsp_gnn/gnngls/model.py", line 75, in forward
    h2 = gnn_layer(graph, h1)
         ^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/nn/pytorch/hetero.py", line 210, in forward
    dstdata = self._get_module((stype, etype, dtype))(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/nn/pytorch/conv/gatconv.py", line 337, in forward
    graph.apply_edges(fn.u_add_v("el", "er", "e"))
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/heterograph.py", line 4696, in apply_edges
    edata = core.invoke_gsddmm(g, func)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/core.py", line 297, in invoke_gsddmm
    z = op(graph, x, y)
        ^^^^^^^^^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/ops/sddmm.py", line 137, in func
    return gsddmm(
           ^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/ops/sddmm.py", line 78, in gsddmm
    return gsddmm_internal(
           ^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/backend/pytorch/sparse.py", line 1046, in gsddmm
    return GSDDMM.apply(*args)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/miniconda3/envs/cuda121/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/backend/pytorch/sparse.py", line 446, in forward
    out = _gsddmm(gidx, op, X, Y, lhs_target, rhs_target)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/_sparse_ops.py", line 552, in _gsddmm
    out = F.empty(out_shp, dtype, ctx)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/p_gnngw/.local/lib/python3.11/site-packages/dgl/backend/pytorch/tensor.py", line 284, in empty
    return th.empty(shape, dtype=dtype, device=ctx)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 59.43 GiB. GPU 0 has a total capacity of 39.43 GiB of which 22.36 GiB is free. Including non-PyTorch memory, this process has 17.07 GiB memory in use. Of the allocated memory 622.66 MiB is allocated by PyTorch, and 11.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
